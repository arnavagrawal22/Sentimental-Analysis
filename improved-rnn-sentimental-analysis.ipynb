{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:30:07.047113Z","iopub.status.busy":"2022-02-12T12:30:07.046800Z","iopub.status.idle":"2022-02-12T12:30:18.180462Z","shell.execute_reply":"2022-02-12T12:30:18.179702Z","shell.execute_reply.started":"2022-02-12T12:30:07.047033Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchtext.legacy import data\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy',\n","                  tokenizer_language = 'en_core_web_sm',\n","                  include_lengths = True) #to make sure we get actual length of batch in return value\n","\n","LABEL = data.LabelField(dtype = torch.float)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:30:18.182534Z","iopub.status.busy":"2022-02-12T12:30:18.182205Z","iopub.status.idle":"2022-02-12T12:31:39.122816Z","shell.execute_reply":"2022-02-12T12:31:39.121942Z","shell.execute_reply.started":"2022-02-12T12:30:18.182497Z"},"trusted":true},"outputs":[],"source":["from torchtext.legacy import datasets\n","\n","train_dataset, test_dataset = datasets.IMDB.splits(TEXT, LABEL)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:31:39.124231Z","iopub.status.busy":"2022-02-12T12:31:39.123991Z","iopub.status.idle":"2022-02-12T12:31:39.165816Z","shell.execute_reply":"2022-02-12T12:31:39.165089Z","shell.execute_reply.started":"2022-02-12T12:31:39.124199Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["25000\n","12500\n","12500\n"]}],"source":["#we need to split the test into val and test data\n","val_dataset,test_dataset = test_dataset.split(0.5)\n","print(len(train_dataset))\n","print(len(test_dataset))\n","print(len(val_dataset))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:31:39.168729Z","iopub.status.busy":"2022-02-12T12:31:39.168115Z","iopub.status.idle":"2022-02-12T12:34:58.686152Z","shell.execute_reply":"2022-02-12T12:34:58.685393Z","shell.execute_reply.started":"2022-02-12T12:31:39.168698Z"},"trusted":true},"outputs":[],"source":["MAX_VOCAB_SIZE = 25000\n","\n","TEXT.build_vocab(train_dataset, \n","                 max_size = MAX_VOCAB_SIZE)\n","                 #needed if you are training the model, as model trained on Kaggle, commenting it\n","                 #vectors = \"glove.6B.100d\", #using pretrained embeddings\n","                 #unk_init = torch.Tensor.normal_) #initializing all vocab, but not in pretrained, to random values\n","\n","LABEL.build_vocab(train_dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:34:58.688677Z","iopub.status.busy":"2022-02-12T12:34:58.688419Z","iopub.status.idle":"2022-02-12T12:34:58.696925Z","shell.execute_reply":"2022-02-12T12:34:58.695119Z","shell.execute_reply.started":"2022-02-12T12:34:58.688643Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_dataset, val_dataset, test_dataset), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True, #for using packed, we need each batch sorted by length\n","    device = device)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:34:58.698660Z","iopub.status.busy":"2022-02-12T12:34:58.698380Z","iopub.status.idle":"2022-02-12T12:34:58.710618Z","shell.execute_reply":"2022-02-12T12:34:58.709952Z","shell.execute_reply.started":"2022-02-12T12:34:58.698624Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx) #PAD INDEX as we \n","        #don't want to learn the embeddings for the paddings\n","        \n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim) #no need for relu or anything as loss will cover it\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths): #we need to give text lenghths in each pass, as we are using packed \n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        #pack sequence\n","        # lengths need to be on CPU!\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n","        #to cpu is in documentation, text_lengths we will get from iterator, and its actual lenght of each sen\n","        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded) #LSTM returns 3 things\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors as they aren't even trained\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) #last two hidden are concat\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:34:58.712405Z","iopub.status.busy":"2022-02-12T12:34:58.711969Z","iopub.status.idle":"2022-02-12T12:34:58.762906Z","shell.execute_reply":"2022-02-12T12:34:58.762060Z","shell.execute_reply.started":"2022-02-12T12:34:58.712370Z"},"trusted":true},"outputs":[],"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:34:58.764595Z","iopub.status.busy":"2022-02-12T12:34:58.764296Z","iopub.status.idle":"2022-02-12T12:34:58.772172Z","shell.execute_reply":"2022-02-12T12:34:58.771446Z","shell.execute_reply.started":"2022-02-12T12:34:58.764560Z"},"trusted":true},"outputs":[],"source":["# putting pretrained embeddings and initializing embedding layer\n","pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","print(pretrained_embeddings.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:34:58.774627Z","iopub.status.busy":"2022-02-12T12:34:58.773692Z","iopub.status.idle":"2022-02-12T12:34:58.814084Z","shell.execute_reply":"2022-02-12T12:34:58.813234Z","shell.execute_reply.started":"2022-02-12T12:34:58.774588Z"},"trusted":true},"outputs":[],"source":["#changing <pad> and <unk> embeddings to zero(Not compolsary)\n","model.embedding.weight.data[0] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[1] = torch.zeros(EMBEDDING_DIM)\n","print(model.embedding.weight.data)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:35:41.617608Z","iopub.status.busy":"2022-02-12T12:35:41.617187Z","iopub.status.idle":"2022-02-12T12:41:15.035073Z","shell.execute_reply":"2022-02-12T12:41:15.033869Z","shell.execute_reply.started":"2022-02-12T12:35:41.617571Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n","criterion = nn.BCEWithLogitsLoss()\n","model = model.to(device)\n","def binary_accuracy(predicted, labels):\n","    actual_prediction = torch.round(torch.sigmoid(predicted))\n","    acc = (actual_prediction == labels).sum().float() / len(labels)\n","    return acc\n","\n","def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in tqdm(iterator):\n","        \n","        optimizer.zero_grad()\n","        \n","        text,text_len = batch.text\n","        predictions = model(text,text_len).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in tqdm(iterator):\n","\n","            text,text_len = batch.text\n","            predictions = model(text,text_len).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","# N_EPOCHS = 50\n","# from tqdm import tqdm \n","\n","# for epoch in tqdm(range(N_EPOCHS)):\n","\n","#     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","#     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)    \n","#     print(f'Epoch: {epoch+1:02}')\n","#     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","#     if (epoch%5 == 0):\n","#         torch.save(model.state_dict(), 'improved-rnn-15-epoch.pt') "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:41:42.781704Z","iopub.status.busy":"2022-02-12T12:41:42.781073Z","iopub.status.idle":"2022-02-12T12:41:50.661611Z","shell.execute_reply":"2022-02-12T12:41:50.660859Z","shell.execute_reply.started":"2022-02-12T12:41:42.781662Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Loaded Successfully\n"]}],"source":["model.load_state_dict(torch.load('improved-rnn-5-epoch.pt', map_location=device))\n","print(\"Model Loaded Successfully\")\n","# valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","# train_loss, train_acc = evaluate(model, test_iterator, criterion)\n","# print(valid_acc)\n","# print(train_acc)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:41:53.977680Z","iopub.status.busy":"2022-02-12T12:41:53.977405Z","iopub.status.idle":"2022-02-12T12:41:54.608146Z","shell.execute_reply":"2022-02-12T12:41:54.607340Z","shell.execute_reply.started":"2022-02-12T12:41:53.977650Z"},"trusted":true},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)] #tokensising it \n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized] #indexing it\n","    length = [len(indexed)] #length of word is needed\n","    tensor = torch.LongTensor(indexed).to(device) #converting it to tensors\n","    tensor = tensor.unsqueeze(1) #adding one dimension for batch size\n","    length_tensor = torch.LongTensor(length)\n","    prediction = torch.sigmoid(model(tensor, length_tensor)) #getting the prediction\n","    return prediction.item()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T12:43:44.122417Z","iopub.status.busy":"2022-02-12T12:43:44.121721Z","iopub.status.idle":"2022-02-12T12:43:44.131947Z","shell.execute_reply":"2022-02-12T12:43:44.131128Z","shell.execute_reply.started":"2022-02-12T12:43:44.122379Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.00918661244213581"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["predict_sentiment(model,\"I m going to write the honest and sincere comments and suggestions for indian audiance who watch most bollywood films. Guys first of all the entire Race Frenchie that is race 1, and 2 both are ripped off from Hollywood flick check out in Google and IMBD that means they are entirely copy pasted the movie even the songs composed by copycat Pritam Chakraborty. Song's we're copied from Korean album my Sasi girl. Now Race 3 what is new .... Nothing there is nothing in this movie which makes you heartwarming, eye catching or any sort of connection with characters in the movie. This movie is also not worth watching for free on television because your valuable time will be wasted and that is equal to loosing MONEY. When this movie got first premiered on television on I suppose on Zee cinema. I watched this for nearly 10 minutes and I felt what the heck I m doing during break i just browsing through channels I came across Hollywood Bean movie. This movie really saved my day and got rid of Race 3. Why people are still praising salman khan why he s now getting aged and he should pass on the battle and let New face come to bollywood but unfortunately it s bollywood Full of nepotism it will never improve it's nepotism, favouritism strategy. Hence request you All please don't waste your time and money on these worthless star's. There are wonderful... Incredible astonishing amazing Hollywood flicks collection you can watch instead of this kind of trash\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
